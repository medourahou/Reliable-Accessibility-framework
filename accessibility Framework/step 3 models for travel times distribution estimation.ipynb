{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af6757da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5cd83fb",
   "metadata": {},
   "source": [
    "## Train Probabilistic ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a61ffcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.special import gammaln\n",
    "import scipy.special\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"Prepare features and target.\"\"\"\n",
    "    features = ['departure_latitude', 'departure_longitude', 'arrival_latitude',\n",
    "                'arrival_longitude', 'departure_time_hour', 'departure_time_minute',\n",
    "                'departure_time_seconds', 'departure_time_day_of_week',\n",
    "                'departure_time_day_of_month', 'departure_time_month',\n",
    "                'departure_time_hour_sin', 'departure_time_hour_cos',\n",
    "                'departure_time_day_of_week_sin', 'departure_time_day_of_week_cos',\n",
    "                'departure_time_month_sin', 'departure_time_month_cos',\n",
    "                'route_distance']\n",
    "    \n",
    "    X = df[features].values\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y = np.maximum(df['travel_time'].values, 1e-6)\n",
    "    return X, y\n",
    "\n",
    "def evaluate_predictions(y_true, predictions):\n",
    "    \"\"\"Calculate only MAE, RMSE, NLL, and KL.\"\"\"\n",
    "    y_pred = predictions['mean']\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    # Basic metrics\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "    # NLL\n",
    "    alpha = (predictions['mean'] / predictions['std'])**2\n",
    "    beta = predictions['mean'] / (predictions['std']**2)\n",
    "    \n",
    "    alpha = np.maximum(alpha, epsilon)\n",
    "    beta = np.maximum(beta, epsilon)\n",
    "    y_true = np.maximum(y_true, epsilon)\n",
    "    \n",
    "    gamma_loglik = (alpha * np.log(beta) - \n",
    "                   scipy.special.gammaln(alpha) + \n",
    "                   (alpha - 1) * np.log(y_true) - \n",
    "                   beta * y_true)\n",
    "    nll = -np.mean(gamma_loglik)\n",
    "\n",
    "    # KL divergence\n",
    "    actual_mean = np.mean(y_true)\n",
    "    actual_var = np.var(y_true)\n",
    "    empirical_alpha = (actual_mean ** 2) / actual_var\n",
    "    empirical_beta = actual_mean / actual_var\n",
    "\n",
    "    predicted_var = predictions['std'] ** 2\n",
    "    predicted_alpha = (predictions['mean'] ** 2) / predicted_var\n",
    "    predicted_beta = predictions['mean'] / predicted_var\n",
    "\n",
    "    empirical_alpha = np.maximum(empirical_alpha, epsilon)\n",
    "    empirical_beta = np.maximum(empirical_beta, epsilon)\n",
    "    predicted_alpha = np.maximum(predicted_alpha, epsilon)\n",
    "    predicted_beta = np.maximum(predicted_beta, epsilon)\n",
    "\n",
    "    kl_div = (empirical_alpha * np.log(empirical_beta / predicted_beta) +\n",
    "              gammaln(predicted_alpha) - gammaln(empirical_alpha) +\n",
    "              (empirical_alpha - predicted_alpha) * scipy.special.digamma(empirical_alpha) +\n",
    "              predicted_alpha * (empirical_beta / predicted_beta - 1))\n",
    "    kl = np.mean(np.maximum(kl_div, 0.0))\n",
    "\n",
    "    return {'mae': mae, 'rmse': rmse, 'nll': nll, 'kl': kl}\n",
    "\n",
    "class BaseModel(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Base model with core metrics.\"\"\"\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict_distribution(X)\n",
    "        metrics = evaluate_predictions(y, predictions)\n",
    "        return {'predictions': predictions, 'metrics': metrics}\n",
    "\n",
    "class LinearRegressionGamma(BaseModel):\n",
    "    \"\"\"Linear regression with Gamma distribution.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "        self.scale = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = X[:, -1] if X.ndim > 1 else X\n",
    "        self.theta = abs(np.sum(X * y) / np.sum(X * X))\n",
    "        mean_pred = self.theta * X\n",
    "        residuals = y - mean_pred\n",
    "        self.scale = np.var(residuals) / np.mean(mean_pred)\n",
    "        return self\n",
    "    \n",
    "    def predict_distribution(self, X, n_samples=4000):\n",
    "        X = X[:, -1] if X.ndim > 1 else X\n",
    "        mean = np.maximum(self.theta * X, 1e-6)\n",
    "        shape = mean / self.scale\n",
    "        \n",
    "        samples = np.random.gamma(shape=shape[:, np.newaxis],\n",
    "                                scale=self.scale,\n",
    "                                size=(len(X), n_samples))\n",
    "        \n",
    "        return {\n",
    "            'predictions': samples,\n",
    "            'mean': mean,\n",
    "            'std': np.std(samples, axis=1)\n",
    "        }\n",
    "\n",
    "class RandomForestRegressorWith_Gamma(BaseModel):\n",
    "    \"\"\"Random Forest based distribution learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100):\n",
    "        self.model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            min_samples_leaf=5,\n",
    "            max_features='sqrt',\n",
    "            bootstrap=True,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict_distribution(self, X, n_samples=4000):\n",
    "        preds = np.array([est.predict(X) for est in self.model.estimators_])\n",
    "        mean = np.mean(preds, axis=0)\n",
    "        std = np.std(preds, axis=0)\n",
    "        \n",
    "        alpha = (mean / std) ** 2\n",
    "        beta = mean / (std ** 2)\n",
    "        samples = np.array([\n",
    "            np.random.gamma(alpha[i], 1/beta[i], n_samples)\n",
    "            for i in range(len(X))\n",
    "        ])\n",
    "        \n",
    "        return {\n",
    "            'predictions': samples.T,\n",
    "            'mean': mean,\n",
    "            'std': std\n",
    "        }\n",
    "\n",
    "class BNNTravelTimeModel(PyroModule):\n",
    "    \"\"\"BNN model with CUDA support.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Initialize parameters\n",
    "        df = torch.tensor(9.815853992105808, dtype=torch.float32).to(device)\n",
    "        loc = torch.tensor(0.0, dtype=torch.float32).to(device)\n",
    "        scale = torch.tensor(1.0044249756892316, dtype=torch.float32).to(device)\n",
    "      \n",
    "        # First hidden layer\n",
    "        self.fc1 = PyroModule[nn.Linear](input_dim, hidden_dim)\n",
    "        self.fc1.weight = PyroSample(\n",
    "            dist.StudentT(df=df, loc=loc, scale=scale)\n",
    "            .expand([hidden_dim, input_dim]).to_event(2)\n",
    "        )\n",
    "        self.fc1.bias = PyroSample(\n",
    "            dist.StudentT(df=df, loc=loc, scale=scale)\n",
    "            .expand([hidden_dim]).to_event(1)\n",
    "        )\n",
    "        \n",
    "        # Output layer with 2 outputs\n",
    "        self.fc2 = PyroModule[nn.Linear](hidden_dim, 2)\n",
    "        self.fc2.weight = PyroSample(\n",
    "            dist.StudentT(df=df, loc=loc, scale=scale)\n",
    "            .expand([2, hidden_dim]).to_event(2)\n",
    "        )\n",
    "        self.fc2.bias = PyroSample(\n",
    "            dist.StudentT(df=df, loc=loc, scale=scale)\n",
    "            .expand([2]).to_event(1)\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        self = self.to(device)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = x.to(device).float()\n",
    "        if y is not None:\n",
    "            y = y.to(device).float()\n",
    "\n",
    "        hidden = self.relu(self.fc1(x))\n",
    "        network_output = self.fc2(hidden)\n",
    "        \n",
    "        log_mean = network_output[..., 0]  \n",
    "        log_shape = self.softplus(network_output[..., 1])  \n",
    "        \n",
    "        mean = torch.exp(log_mean)\n",
    "        shape = torch.exp(log_shape) + 1.0\n",
    "        \n",
    "        rate = torch.clamp(shape / mean, min=1e-3, max=100.0)\n",
    "        shape = mean * rate\n",
    "        shape = torch.clamp(shape, min=1.0)\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            if y is not None:\n",
    "                y_orig = torch.exp(y)\n",
    "            else:\n",
    "                y_orig = None\n",
    "                \n",
    "            obs = pyro.sample(\n",
    "                \"obs\",\n",
    "                dist.Gamma(shape, rate),\n",
    "                obs=y_orig\n",
    "            )\n",
    "\n",
    "        return mean\n",
    "\n",
    "class BNNPredictor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"BNN-based travel time predictor with CUDA support.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=16):\n",
    "        pyro.clear_param_store()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.feature_names = [\n",
    "            'departure_latitude', 'departure_longitude', 'arrival_latitude',\n",
    "            'arrival_longitude', 'departure_time_hour', 'departure_time_minute',\n",
    "            'departure_time_seconds', 'departure_time_day_of_week',\n",
    "            'departure_time_day_of_month', 'departure_time_month',\n",
    "            'departure_time_hour_sin', 'departure_time_hour_cos',\n",
    "            'departure_time_day_of_week_sin', 'departure_time_day_of_week_cos',\n",
    "            'departure_time_month_sin', 'departure_time_month_cos',\n",
    "            'route_distance'\n",
    "        ]\n",
    "                    \n",
    "        if input_dim != len(self.feature_names):\n",
    "            raise ValueError(f\"input_dim ({input_dim}) must match number of features ({len(self.feature_names)})\")\n",
    "            \n",
    "        self.model = BNNTravelTimeModel(input_dim, hidden_dim)\n",
    "        self.model.to(self.device)\n",
    "        self.guide = pyro.infer.autoguide.AutoDiagonalNormal(self.model)\n",
    "        self.guide.to(self.device)\n",
    "        self.svi = None\n",
    "        self.feature_means = None\n",
    "        self.feature_stds = None\n",
    "\n",
    "    def prepare_data(self, X, y=None):\n",
    "        \"\"\"Prepare data for training or prediction.\"\"\"\n",
    "        # Convert to DataFrame if numpy array\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=self.feature_names)\n",
    "            \n",
    "        features = X[self.feature_names].values\n",
    "        features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # Initialize scalers during training\n",
    "        if y is not None and self.feature_means is None:\n",
    "            self.feature_means = np.mean(features, axis=0)\n",
    "            self.feature_stds = np.std(features, axis=0)\n",
    "            self.feature_stds[self.feature_stds == 0] = 1.0\n",
    "        \n",
    "        if self.feature_means is None:\n",
    "            raise ValueError(\"Scalers not initialized. Need to run training first.\")\n",
    "        \n",
    "        # Scale features\n",
    "        features = (features - self.feature_means) / self.feature_stds\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        if y is not None:\n",
    "            y = np.maximum(y, 1e-6)\n",
    "            log_y = np.log(y)\n",
    "            y_tensor = torch.tensor(log_y, dtype=torch.float32).to(self.device)\n",
    "            return features_tensor, y_tensor\n",
    "\n",
    "        return features_tensor\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        features, target = self.prepare_data(X, y)\n",
    "        \n",
    "        dataset = TensorDataset(features, target)\n",
    "        train_loader = DataLoader(dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.01})\n",
    "        self.svi = SVI(self.model, self.guide, optimizer, loss=Trace_ELBO())\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        best_state_dict = None\n",
    "        \n",
    "        try:\n",
    "            for _ in range(100):\n",
    "                \n",
    "                perm = torch.randperm(len(features), device=self.device)\n",
    "                shuffled_features = features[perm]\n",
    "                shuffled_target = target[perm]\n",
    "                \n",
    "                shuffled_dataset = TensorDataset(shuffled_features, shuffled_target)\n",
    "                loader = DataLoader(shuffled_dataset, batch_size=1024)\n",
    "                \n",
    "                epoch_losses = []\n",
    "                for batch_X, batch_y in loader:\n",
    "                    batch_X = batch_X.to(self.device)\n",
    "                    batch_y = batch_y.to(self.device)\n",
    "                    loss = self.svi.step(batch_X, batch_y)\n",
    "                    epoch_losses.append(loss)\n",
    "                \n",
    "                avg_loss = np.mean(epoch_losses)\n",
    "                if avg_loss < best_loss:\n",
    "                    best_loss = avg_loss\n",
    "                    best_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "            \n",
    "            if best_state_dict is not None:\n",
    "                self.model.load_state_dict(best_state_dict)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Training error: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_distribution(self, X, n_samples=4000):\n",
    "        \"\"\"Generate probabilistic predictions.\"\"\"\n",
    "        features = self.prepare_data(X)\n",
    "        \n",
    "        try:\n",
    "            predictive = pyro.infer.Predictive(self.model, guide=self.guide, num_samples=n_samples)\n",
    "            with torch.no_grad():\n",
    "                predictions = predictive(features)\n",
    "            samples = predictions[\"obs\"].cpu().detach().numpy()\n",
    "            \n",
    "            return {\n",
    "                'predictions': samples,\n",
    "                'mean': np.mean(samples, axis=0),\n",
    "                'std': np.std(samples, axis=0)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make point predictions.\"\"\"\n",
    "        return self.predict_distribution(X)['mean']\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluate model performance.\"\"\"\n",
    "        predictions = self.predict_distribution(X)\n",
    "        epsilon = 1e-8\n",
    "        \n",
    "        # Basic metrics\n",
    "        mae = np.mean(np.abs(predictions['mean'] - y))\n",
    "        rmse = np.sqrt(np.mean((predictions['mean'] - y) ** 2))\n",
    "\n",
    "        # NLL calculation\n",
    "        alpha = (predictions['mean'] / predictions['std'])**2\n",
    "        beta = predictions['mean'] / (predictions['std']**2)\n",
    "        \n",
    "        alpha = np.maximum(alpha, epsilon)\n",
    "        beta = np.maximum(beta, epsilon)\n",
    "        y = np.maximum(y, epsilon)\n",
    "        \n",
    "        gamma_loglik = (alpha * np.log(beta) - \n",
    "                       scipy.special.gammaln(alpha) + \n",
    "                       (alpha - 1) * np.log(y) - \n",
    "                       beta * y)\n",
    "        nll = -np.mean(gamma_loglik)\n",
    "\n",
    "        # KL divergence\n",
    "        actual_mean = np.mean(y)\n",
    "        actual_var = np.var(y)\n",
    "        empirical_alpha = (actual_mean ** 2) / actual_var\n",
    "        empirical_beta = actual_mean / actual_var\n",
    "\n",
    "        predicted_var = predictions['std'] ** 2\n",
    "        predicted_alpha = (predictions['mean'] ** 2) / predicted_var\n",
    "        predicted_beta = predictions['mean'] / predicted_var\n",
    "\n",
    "        empirical_alpha = np.maximum(empirical_alpha, epsilon)\n",
    "        empirical_beta = np.maximum(empirical_beta, epsilon)\n",
    "        predicted_alpha = np.maximum(predicted_alpha, epsilon)\n",
    "        predicted_beta = np.maximum(predicted_beta, epsilon)\n",
    "\n",
    "        kl = (empirical_alpha * np.log(empirical_beta / predicted_beta) +\n",
    "              scipy.special.gammaln(predicted_alpha) -\n",
    "              scipy.special.gammaln(empirical_alpha) +\n",
    "              (empirical_alpha - predicted_alpha) * scipy.special.digamma(empirical_alpha) +\n",
    "              predicted_alpha * (empirical_beta / predicted_beta - 1))\n",
    "        kl = np.mean(np.maximum(kl, 0.0))\n",
    "\n",
    "        return {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'nll': nll,\n",
    "            'kl': kl,\n",
    "            'predictions': predictions\n",
    "        }\n",
    "class StableGammaKDE:\n",
    "    \"\"\"Gamma KDE implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, bandwidth='scott'):\n",
    "        self.bandwidth_method = bandwidth\n",
    "        self.bandwidth = None\n",
    "        self.samples = None\n",
    "    \n",
    "    def fit(self, x):\n",
    "        self.samples = np.maximum(x.reshape(-1), 1e-6)\n",
    "        self.scale_factor = np.median(self.samples)\n",
    "        self.samples = self.samples / self.scale_factor\n",
    "        \n",
    "        if self.bandwidth_method == 'scott':\n",
    "            n = len(self.samples)\n",
    "            sigma = np.std(self.samples)\n",
    "            self.bandwidth = sigma * np.power(n, -1/5)\n",
    "        else:\n",
    "            self.bandwidth = self.bandwidth_method\n",
    "        return self\n",
    "    \n",
    "    def sample(self, n_samples):\n",
    "        idx = np.random.randint(0, len(self.samples), n_samples)\n",
    "        base_samples = self.samples[idx]\n",
    "        shape = np.maximum(base_samples/self.bandwidth, 1e-6)\n",
    "        scale = self.bandwidth\n",
    "        samples = np.random.gamma(shape, scale)\n",
    "        return np.maximum(samples * self.scale_factor, 1e-6)\n",
    "\n",
    "class ConditionalGammaKDE(BaseModel):\n",
    "    \"\"\"Conditional Gamma KDE model.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters=50, bandwidth='scott'):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.bandwidth = bandwidth\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.clusterer = KMeans(n_clusters=n_clusters)\n",
    "        self.kdes = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        y = np.asarray(y, dtype=np.float64)\n",
    "        \n",
    "        X_scaled = self.feature_scaler.fit_transform(X)\n",
    "        y_positive = np.maximum(y, 1e-6)\n",
    "        \n",
    "        self.clusterer.fit(X_scaled)\n",
    "        clusters = self.clusterer.predict(X_scaled)\n",
    "        \n",
    "        for cluster_id in range(self.n_clusters):\n",
    "            cluster_mask = (clusters == cluster_id)\n",
    "            if np.sum(cluster_mask) > 10:\n",
    "                cluster_times = y_positive[cluster_mask]\n",
    "                kde = StableGammaKDE(bandwidth=self.bandwidth)\n",
    "                kde.fit(cluster_times)\n",
    "                self.kdes[cluster_id] = kde\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def predict_distribution(self, X, n_samples=4000):\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        X_scaled = self.feature_scaler.transform(X)\n",
    "        clusters = self.clusterer.predict(X_scaled)\n",
    "        samples = np.zeros((len(X), n_samples))\n",
    "        \n",
    "        for i, cluster_id in enumerate(clusters):\n",
    "            if cluster_id in self.kdes:\n",
    "                samples[i] = self.kdes[cluster_id].sample(n_samples)\n",
    "            else:\n",
    "                distances = np.linalg.norm(\n",
    "                    self.clusterer.cluster_centers_ - X_scaled[i], \n",
    "                    axis=1\n",
    "                )\n",
    "                nearest_cluster = np.argmin(distances)\n",
    "                samples[i] = self.kdes[nearest_cluster].sample(n_samples)\n",
    "        \n",
    "        return {\n",
    "            'predictions': samples,\n",
    "            'mean': np.mean(samples, axis=1),\n",
    "            'std': np.std(samples, axis=1)\n",
    "        }\n",
    "\n",
    "def train_and_evaluate(model, train_df, test_df, model_name=\"Model\"):\n",
    "    \"\"\"Train and evaluate model with core metrics.\"\"\"\n",
    "    X_train, y_train = prepare_data(train_df)\n",
    "    X_test, y_test = prepare_data(test_df)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.evaluate(X_test, y_test)\n",
    "    metrics = predictions['metrics']\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"MAE: {metrics['mae']:.2f}\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.2f}\")\n",
    "    print(f\"NLL: {metrics['nll']:.4f}\")\n",
    "    print(f\"KL: {metrics['kl']:.4f}\")\n",
    "    \n",
    "    return model, predictions\n",
    "\n",
    "\n",
    "\n",
    "def run_all_models(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Run all models and return both trained models and their results.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (trained_models_dict, results_dict)\n",
    "            - trained_models_dict: Dictionary with model names as keys and trained model objects as values\n",
    "            - results_dict: Dictionary with model names as keys and evaluation results as values\n",
    "    \"\"\"\n",
    "    initial_models = {\n",
    "        'Linear Regression': LinearRegressionGamma(),\n",
    "        'RandomForestRegressorWith_Gamma': RandomForestRegressorWith_Gamma(n_estimators=100),\n",
    "        'BNN': BNNPredictor(input_dim=17, hidden_dim=32),\n",
    "        'Conditional KDE': ConditionalGammaKDE(n_clusters=50)\n",
    "    }\n",
    "    \n",
    "    trained_models = {}\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in initial_models.items():\n",
    "        # Train and evaluate\n",
    "        trained_model, preds = train_and_evaluate(model, train_df, test_df, name)\n",
    "        \n",
    "        # Store both the trained model and results\n",
    "        trained_models[name] = trained_model\n",
    "        results[name] = preds\n",
    "        \n",
    "    return trained_models, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f84fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "legs_input_path = '../Cities/Padam_terretory_01/Ressources/cleaned_padam__drt_trips_terretory_01.csv' \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "crs_import = 2154 #2154 # crs of the raw imported dat\n",
    "crs_working = 2154 \n",
    "\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor' if torch.cuda.is_available() else 'torch.FloatTensor')\n",
    "pyro.set_rng_seed(0)\n",
    "\n",
    "# Toggle for preprocessing steps\n",
    "# GTFS preprocessing to get hexagonal grid\n",
    "#      --> needed once, can be skipped if GTFScells.csv file exists\n",
    "toggle_GTFSprep = False\n",
    "GTFScells_path = '../Cities/Padam_terretory_01/Ressources/cells_of_padam_terretory_01.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1fb9706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1836932/112659549.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset.rename(columns = {   \"departure_latitude_x\" : 'departure_latitude',\n"
     ]
    }
   ],
   "source": [
    "trips_df= pd.read_csv(legs_input_path, sep=\",\")\n",
    "dataset = trips_df[['departure_latitude_x', 'departure_longitude_x', 'arrival_latitude_x',\n",
    "       'arrival_longitude_x', 'departure_time_hour',\n",
    "       'departure_time_minute', 'departure_time_seconds',\n",
    "       'departure_time_day_of_week', 'departure_time_day_of_month',\n",
    "       'departure_time_month', 'departure_time_hour_sin',\n",
    "       'departure_time_hour_cos', 'departure_time_day_of_week_sin',\n",
    "       'departure_time_day_of_week_cos', 'departure_time_month_sin',\n",
    "       'departure_time_month_cos', 'route_distance','travel_time']]\n",
    "\n",
    "dataset.rename(columns = {   \"departure_latitude_x\" : 'departure_latitude',\n",
    "                            \"departure_longitude_x\" : \"departure_longitude\",\n",
    "                            \"arrival_latitude_x\" : 'arrival_latitude',\n",
    "                            \"arrival_longitude_x\" : 'arrival_longitude'\n",
    "                        \n",
    "                          },inplace=True)\n",
    "\n",
    "dataset = dataset[dataset[\"travel_time\"]<=2500] #consider only travel times less than 40min\n",
    "dataset =dataset[dataset[\"travel_time\"]>=60] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "759090f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "models, results = run_all_models(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad28c8",
   "metadata": {},
   "source": [
    "## Estimate travel times threshold between centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "549dcc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>centroid_x_departure</th>\n",
       "      <th>centroid_y_departure</th>\n",
       "      <th>centroid_x_arrival</th>\n",
       "      <th>centroid_y_arrival</th>\n",
       "      <th>route_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.826646</td>\n",
       "      <td>48.850469</td>\n",
       "      <td>1.826646</td>\n",
       "      <td>48.858257</td>\n",
       "      <td>1097.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.826646</td>\n",
       "      <td>48.850469</td>\n",
       "      <td>1.826646</td>\n",
       "      <td>48.866044</td>\n",
       "      <td>1864.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.826646</td>\n",
       "      <td>48.850469</td>\n",
       "      <td>1.826646</td>\n",
       "      <td>48.873832</td>\n",
       "      <td>3110.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.826646</td>\n",
       "      <td>48.850469</td>\n",
       "      <td>1.826646</td>\n",
       "      <td>48.881619</td>\n",
       "      <td>5822.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.826646</td>\n",
       "      <td>48.850469</td>\n",
       "      <td>1.826646</td>\n",
       "      <td>48.889406</td>\n",
       "      <td>6846.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58801</th>\n",
       "      <td>2.050419</td>\n",
       "      <td>48.902673</td>\n",
       "      <td>2.051471</td>\n",
       "      <td>48.897194</td>\n",
       "      <td>3063.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58802</th>\n",
       "      <td>2.050705</td>\n",
       "      <td>48.902693</td>\n",
       "      <td>2.051471</td>\n",
       "      <td>48.904981</td>\n",
       "      <td>562.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58803</th>\n",
       "      <td>2.022691</td>\n",
       "      <td>48.914708</td>\n",
       "      <td>2.051471</td>\n",
       "      <td>48.912769</td>\n",
       "      <td>2138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58804</th>\n",
       "      <td>2.050953</td>\n",
       "      <td>48.925025</td>\n",
       "      <td>2.051471</td>\n",
       "      <td>48.920556</td>\n",
       "      <td>331.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58805</th>\n",
       "      <td>2.050347</td>\n",
       "      <td>48.925691</td>\n",
       "      <td>2.051471</td>\n",
       "      <td>48.928344</td>\n",
       "      <td>768.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58806 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       centroid_x_departure  centroid_y_departure  centroid_x_arrival  \\\n",
       "0                  1.826646             48.850469            1.826646   \n",
       "1                  1.826646             48.850469            1.826646   \n",
       "2                  1.826646             48.850469            1.826646   \n",
       "3                  1.826646             48.850469            1.826646   \n",
       "4                  1.826646             48.850469            1.826646   \n",
       "...                     ...                   ...                 ...   \n",
       "58801              2.050419             48.902673            2.051471   \n",
       "58802              2.050705             48.902693            2.051471   \n",
       "58803              2.022691             48.914708            2.051471   \n",
       "58804              2.050953             48.925025            2.051471   \n",
       "58805              2.050347             48.925691            2.051471   \n",
       "\n",
       "       centroid_y_arrival  route_distance  \n",
       "0               48.858257          1097.4  \n",
       "1               48.866044          1864.0  \n",
       "2               48.873832          3110.8  \n",
       "3               48.881619          5822.8  \n",
       "4               48.889406          6846.4  \n",
       "...                   ...             ...  \n",
       "58801           48.897194          3063.7  \n",
       "58802           48.904981           562.4  \n",
       "58803           48.912769          2138.0  \n",
       "58804           48.920556           331.8  \n",
       "58805           48.928344           768.8  \n",
       "\n",
       "[58806 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the centroids file\n",
    "data = pd.read_csv(\"../Cities/Padam_terretory_01/Ressources/final_ready_test_data_centroids.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "968eab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "def add_departure_times(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add multiple departure times to each entry in the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with repeated entries for each departure time\n",
    "    \"\"\"\n",
    "    departure_times = [\n",
    "        '2024-12-01 07:00:00', '2024-12-01 10:00:00', '2024-12-01 12:00:00',\n",
    "        '2024-12-01 17:00:00', '2024-12-01 20:00:00', '2024-12-02 07:00:00',\n",
    "        '2024-12-02 10:00:00', '2024-12-02 12:00:00', '2024-12-02 17:00:00',\n",
    "        '2024-12-02 20:00:00'\n",
    "    ]\n",
    "    \n",
    "    departure_times = [pd.to_datetime(time) for time in departure_times]\n",
    "    dfs = [df.assign(departure_time=time) for time in departure_times]\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add time-based features to the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with departure_time column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with additional time features\n",
    "    \"\"\"\n",
    "    # Basic time features\n",
    "    df['departure_time_hour'] = df['departure_time'].dt.hour\n",
    "    df['departure_time_minute'] = df['departure_time'].dt.minute\n",
    "    df['departure_time_seconds'] = df['departure_time'].dt.second\n",
    "    df['departure_time_day_of_week'] = df['departure_time'].dt.dayofweek\n",
    "    df['departure_time_day_of_month'] = df['departure_time'].dt.day\n",
    "    df['departure_time_month'] = df['departure_time'].dt.month\n",
    "    \n",
    "    # Cyclical features\n",
    "    df['departure_time_hour_sin'] = np.sin(2 * np.pi * df['departure_time_hour']/24)\n",
    "    df['departure_time_hour_cos'] = np.cos(2 * np.pi * df['departure_time_hour']/24)\n",
    "    df['departure_time_day_of_week_sin'] = np.sin(2 * np.pi * df['departure_time_day_of_week']/7)\n",
    "    df['departure_time_day_of_week_cos'] = np.cos(2 * np.pi * df['departure_time_day_of_week']/7)\n",
    "    df['departure_time_month_sin'] = np.sin(2 * np.pi * df['departure_time_month']/12)\n",
    "    df['departure_time_month_cos'] = np.cos(2 * np.pi * df['departure_time_month']/12)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare and select features for the model.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with selected and renamed features\n",
    "    \"\"\"\n",
    "    # Rename columns\n",
    "    column_mapping = {\n",
    "        'centroid_x_departure': 'departure_longitude',\n",
    "        'centroid_y_departure': 'departure_latitude',\n",
    "        'centroid_x_arrival': 'arrival_longitude',\n",
    "        'centroid_y_arrival': 'arrival_latitude'\n",
    "    }\n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Select features in correct order\n",
    "    selected_features = [\n",
    "        'departure_latitude', 'departure_longitude', 'arrival_latitude',\n",
    "        'arrival_longitude', 'departure_time_hour', 'departure_time_minute',\n",
    "        'departure_time_seconds', 'departure_time_day_of_week',\n",
    "        'departure_time_day_of_month', 'departure_time_month',\n",
    "        'departure_time_hour_sin', 'departure_time_hour_cos',\n",
    "        'departure_time_day_of_week_sin', 'departure_time_day_of_week_cos',\n",
    "        'departure_time_month_sin', 'departure_time_month_cos',\n",
    "        'route_distance'\n",
    "    ]\n",
    "    \n",
    "    return df[selected_features]\n",
    "\n",
    "def predict_thresholds(new_data,model, confidence_levels=[0.95, 0.50]):\n",
    "    \"\"\"\n",
    "    Predict travel time thresholds for given confidence levels.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = model.predict_distribution_with_stats(new_data)\n",
    "    pred_samples = predictions['predictions']\n",
    "\n",
    "    thresholds = {}\n",
    "    for conf in confidence_levels:\n",
    "        upper_percentile = 100 * (1 + conf) / 2\n",
    "        thresholds[conf] = {\n",
    "            'threshold': np.percentile(pred_samples, upper_percentile, axis=0),\n",
    "            'mean': predictions['mean'],\n",
    "            'std': predictions['std']\n",
    "        }\n",
    "\n",
    "    return thresholds\n",
    "def predict_travel_times(df: pd.DataFrame, predictor, features: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate travel time predictions for each day and hour combination.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        predictor: Model predictor object\n",
    "        features: List of feature names\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing predictions for each day/hour combination\n",
    "    \"\"\"\n",
    "    days = df['departure_time_day_of_month'].unique()\n",
    "    hours = df['departure_time_hour'].unique()\n",
    "    daily_hourly_predictions = {}\n",
    "    \n",
    "    for day in days:\n",
    "        print(f\"Processing day {day}\")\n",
    "        daily_hourly_predictions[day] = {}\n",
    "        \n",
    "        for hour in hours:\n",
    "            print(f\"Predicting for day {day}, hour {hour}\")\n",
    "            \n",
    "            day_hour_data = df[\n",
    "                (df['departure_time_day_of_month'] == day) & \n",
    "                (df['departure_time_hour'] == hour)\n",
    "            ]\n",
    "            \n",
    "            if len(day_hour_data) > 0:\n",
    "                thresholds =predict_thresholds(predictor,\n",
    "                    day_hour_data[features],\n",
    "                    confidence_levels=[0.95, 0.70, 0.50, 0.25]\n",
    "                )\n",
    "                daily_hourly_predictions[day][hour] = thresholds\n",
    "            else:\n",
    "                print(f\"No data for day {day}, hour {hour}\")\n",
    "                daily_hourly_predictions[day][hour] = None\n",
    "                \n",
    "    return daily_hourly_predictions\n",
    "\n",
    "def assign_thresholds(df: pd.DataFrame, predictions: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign predicted thresholds to the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        predictions: Dictionary of predictions\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with assigned thresholds\n",
    "    \"\"\"\n",
    "    df['travel_time_95'] = None\n",
    "    df['travel_time_70'] = None\n",
    "    df['travel_time_50'] = None\n",
    "    df['travel_time_25'] = None\n",
    "    \n",
    "    for day in predictions.keys():\n",
    "        for hour in predictions[day].keys():\n",
    "            if predictions[day][hour] is None:\n",
    "                continue\n",
    "                \n",
    "            day_hour_mask = (df['departure_time_day_of_month'] == day) & \\\n",
    "                           (df['departure_time_hour'] == hour)\n",
    "            day_hour_indices = df[day_hour_mask].index\n",
    "            n_predictions = len(day_hour_indices)\n",
    "            \n",
    "            if n_predictions > 0:\n",
    "                confidence_levels = [0.95, 0.70, 0.50, 0.25]\n",
    "                threshold_columns = ['travel_time_95', 'travel_time_70', \n",
    "                                  'travel_time_50', 'travel_time_25']\n",
    "                \n",
    "                for conf, col in zip(confidence_levels, threshold_columns):\n",
    "                    df.loc[day_hour_indices, col] = \\\n",
    "                        predictions[day][hour][conf]['threshold'][:n_predictions]\n",
    "                \n",
    "    return df\n",
    "\n",
    "\n",
    "def process_data(data: pd.DataFrame, predictor) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process data through the complete pipeline.\n",
    "    \n",
    "    Args:\n",
    "        data: Input DataFrame\n",
    "        predictor: Model predictor object\n",
    "        \n",
    "    Returns:\n",
    "        Processed DataFrame with predictions\n",
    "    \"\"\"\n",
    "    # Define the features list at the module level\n",
    "    FEATURES = [\n",
    "        # Geographic coordinates\n",
    "        'departure_latitude', 'departure_longitude',\n",
    "        'arrival_latitude', 'arrival_longitude',\n",
    "\n",
    "        # Raw time features\n",
    "        'departure_time_hour',\n",
    "        'departure_time_minute',\n",
    "        'departure_time_seconds',\n",
    "        'departure_time_day_of_week',\n",
    "        'departure_time_day_of_month',\n",
    "        'departure_time_month',\n",
    "\n",
    "        # Cyclical time features\n",
    "        'departure_time_hour_sin',\n",
    "        'departure_time_hour_cos',\n",
    "        'departure_time_day_of_week_sin',\n",
    "        'departure_time_day_of_week_cos',\n",
    "        'departure_time_month_sin',\n",
    "        'departure_time_month_cos',\n",
    "\n",
    "        'route_distance'\n",
    "    ]\n",
    "    # Add departure times and features\n",
    "    data = add_departure_times(data)\n",
    "    data = add_time_features(data)\n",
    "    data = prepare_features(data)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = predict_travel_times(data, predictor, FEATURES)\n",
    "    \n",
    "    # Assign predictions to dataframe\n",
    "    data = assign_thresholds(data, predictions)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5182e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=process_data(data,models[\"RandomForestRegressorWith_Gamma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e10537",
   "metadata": {},
   "source": [
    "##  Create GTFS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47fdc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.create_and_join_gtfs import (\n",
    "    create_gtfs_from_trips,\n",
    "    save_gtfs_files,\n",
    "    joinGTFS,\n",
    "    exportGTFS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Using default threshold (travel_time_25)\n",
    "gtfs_data = create_gtfs_from_trips(data,\"travel_time_25\")\n",
    "\n",
    "# Or specifying a different threshold\n",
    "gtfs_data_25 = create_gtfs_from_trips(data, travel_time_threshold='travel_time_25')\n",
    "\n",
    "# Save with default threshold\n",
    "save_gtfs_files(gtfs_data, \"path/to/output\")\n",
    "\n",
    "# Or save with specific threshold\n",
    "save_gtfs_files(gtfs_data_95, \"path/to/output\", travel_time_threshold='travel_time_25')\n",
    "\n",
    "# For joining and exporting GTFS files:\n",
    "merged_gtfs = joinGTFS(gtfs_dataset1, gtfs_dataset2)\n",
    "exportGTFS(\"path/to/output\", merged_gtfs)  # These functions don't need the threshold parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f25fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3a69e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85a993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
